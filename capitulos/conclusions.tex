\chapter{Conclusions} \label{cap:conclusions}

\section{Conclusions}
All along this undergraduate thesis we have iterated through numerous versions with different approaches, all applied to the generation of our bot for Ms. Pac-Man with grammatical evolution.
 
First of all, with the automaton approach, the bot executed sequences of actions (which was later improved with high level actions and very simple conditional evaluations), even if it’s results weren’t so good score-wise, achieved to develop a tactic that allowed it to progress many levels against the most silly ghosts.
 
After that we oriented our grammars to follow a reactive approach, changing our previous sequences of actions for decision trees. This change was a significative breakthrough, that allowed us to design grammars with different levels of abstraction.
 
Next, we did a series of studies (of selective pressure, codon usage, and fitness functions) which lead us numerous improvements in a try to obtain better results, both in algorithm efficacy (LHS cross-over, neutral mutation, multi-objective optimization, and multiple minor changes to the JECO framework), and the friendliness of our tool (multithreading). All this upgrades had enough repercussion in the bot performance as to include them in the parameters of execution for the best results we show.
 
Finally, after analysing all the tests we did, we could conclude various things.
 
First of all, we obtain the best results, for any grammar and against any ghost controller, using the following parameters for the evolutive algorithm:
\begin{table}[H]
\centering
\begin{tabular}{lcc}
\cline{3-3}
                                                   &                                                               & \textbf{Percentage} \\ \cline{3-3} 
\multicolumn{1}{|l|}{\textbf{Selection operator}} & Binary tournament \footnotemark & -                   \\
\multicolumn{1}{|l|}{\textbf{Crossover operator}}     & LHS                                                           & 60                  \\
\multicolumn{1}{|l|}{\textbf{Mutation operator}}  & Integer Flip                                                  & 10                  \\
\multicolumn{1}{|l|}{\textbf{Neutral Mutation}}    & Yes                                                            & -                   \\
\multicolumn{1}{|l|}{\textbf{Elitism}}            & Yes                                                            & 5                  
\end{tabular}
\end{table}
\footnotetext{or NSGA II if multi-objective is being used}

Second, as Figure~\ref{graph:all_fitness} shows, the bots obtain better results, when using only a few generations (for example 50), as the more abstract the used grammar is. However, when using more generations (for example 100), the bots using the medium-level grammar obtain the best results, due to its higher potencial, as shown later at Table~\ref{table:single_obj}.
 
Third, as Table~\ref{table:single_obj} shows, our grammars can produce bots that play better than the baseline controllers (a random controller and other one that always goes towards the closest pill). Besides, our bots play better than the UCD Dublin bot \cite{galvan2010evolving}, and that is interesting because all of them are created using Grammatical Evolution. This seems to happen due to the usage of excessively specific functions which limit its behaviour, i.e. forcing Pac-Man to wait next to a power pill. Their bot also uses large amount of parameters like dimensions of a frame (centered on Pac-Man) to evaluate certain conditions. Conversely, we make use of path distances and numeric operators, resulting in a less complex game status analysis.

Lastly, even if the use of multi-objective optimization makes short executions obtain slightly better results (it helps to avoid local minimums), it doesn’t obtain a significative difference in long enough executions, as Table~\ref{table:multi-objective} shows (where MO means that the bot uses Multiobjetive Optimization). This happens in Ms. Pac-Man vs Ghosts due to the direct relation between the different fitnesses which can be applied (ghosts eaten, levels cleared..) and the overall game score, which is based on all those.

\section{Distribution}
Considering the impact the project might have, we decided to publish it as open source under GPL \cite{licenseThesisGit} license, on GitHub. We did so hoping to assist any project related to grammatical evolution or artificial intelligence applied to videogames. 

Finally, once we had obtained and analyzed the results of the project, we decided to take it a step further and publish a scientific article about it. It is titled ``A Pac-Man bot based on Grammatical evolution'', and it focuses on the final implementation (decision trees) and the usage of medium and high level grammars, also mentioning the multi-objective optimization. At this moment, the article has been sent to CoSECiVi 2017 (Congress of the Spanish Society for Video Game Sciences) and it is pending peer review.

\section{Future Work}
Although we were satisfied with the extend of our work and it's rsults, we would've liked to experiment and try some other techniques, but we couldn't because we lacked the time. Those techniques are the following:

\subsection{Behaviour Trees}
One of the possibly aplicable techniques are behaviour trees.

The use of behaviour trees woul'dve supposed to add typical loop structures to our grammars (while, for, do-while ...). This would mean that to obtain a terminal node which returns a movement in our decision tree, the controller doesn't always begins from the root of the tree, but it can continue to parse the tree from a point defined by previous movement requests.

This technique would allow to produce more specific strategies that requiere continuity. Without behaviour trees this can still be achieved, but the complexity of conditional evaluations and the size of the tree that would allow it are not realistic in terms of computational power.


\subsection{NEAT}

Another promising technique would be the application of neural networks.

As inputs we would've the same functions that our grammars use to know about the game state, and as output we'd have the movement to execute.

The problem we'd have to face is the network architecture, and for this we'd like to preserve the evolutionary spirit and use the NEAT (NeuroEvolution of Augmenting Topologies) algorithm. This algorithm uses evolutive programming techniques to evolve and protect good neural network topologies until they are sufficiently trained to solve a concrete problem. We think we could add this feature to the JECO framework pretty easily.
